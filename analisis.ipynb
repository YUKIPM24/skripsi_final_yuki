{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795308d8",
   "metadata": {},
   "source": [
    "# MERGE ANALISIS LEXICON DAN MANUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ae9394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>malu sama kaleng sarden dan hongguan yg bisa t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Berguna kagak, membodohi iya.. ðŸ’€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Berhentikan rangka esafðŸ¤¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>kuning2 di rangka esaf itu EMAS TAI, bukan KARAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Mana penjelasan honda jngn lepas tangan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        description\n",
       "0   1  malu sama kaleng sarden dan hongguan yg bisa t...\n",
       "1   2                   Berguna kagak, membodohi iya.. ðŸ’€\n",
       "2   3                           Berhentikan rangka esafðŸ¤¬\n",
       "3   4   kuning2 di rangka esaf itu EMAS TAI, bukan KARAT\n",
       "4   5            Mana penjelasan honda jngn lepas tangan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Klarifikasi_Kemunculan_Warna_Kuning_Pada_Rangka_Honda.xlsx\")\n",
    "\n",
    "df = df[[\"id\", \"description\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a7378d-68bd-448d-bfe5-42c65d8527d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>malu sama kaleng sarden dan hongguan yg bisa t...</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Berguna kagak, membodohi iya.. ðŸ’€</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Berhentikan rangka esafðŸ¤¬</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kuning2 di rangka esaf itu EMAS TAI, bukan KARAT</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mana penjelasan honda jngn lepas tangan</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment_label\n",
       "0  malu sama kaleng sarden dan hongguan yg bisa t...         Negatif\n",
       "1                   Berguna kagak, membodohi iya.. ðŸ’€         Negatif\n",
       "2                           Berhentikan rangka esafðŸ¤¬         Negatif\n",
       "3   kuning2 di rangka esaf itu EMAS TAI, bukan KARAT         Negatif\n",
       "4            Mana penjelasan honda jngn lepas tangan         Negatif"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_manual = pd.read_excel(\"labeling manual.xlsx\")\n",
    "df_manual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5eca5",
   "metadata": {},
   "source": [
    "# Cleansing\n",
    "Menghapus tanda baca,url, spasi dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90660045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expression (regex)\n",
    "import re\n",
    "import string\n",
    "\n",
    "def cleaning(Text, replacements=None):\n",
    "    if pd.isnull(Text):  # Check if Text is NaN\n",
    "        return \"\"\n",
    "    # Mengganti huruf yang berulang-ulang ('oooooo' menjadi '00')\n",
    "    Text = re.sub(r'(.)\\1+', r'\\1\\1', Text)\n",
    "    # Mengganti 2 atau lebih titik dengan spasi\n",
    "    Text = re.sub(r'\\.{2,}', ' ', Text)\n",
    "    # Menghapus @username\n",
    "    Text = re.sub('@[^\\s]+','', Text)\n",
    "    # Menghapus angka\n",
    "    Text = re.sub('[0-9]+', '', Text)\n",
    "    # Menghapus URL\n",
    "    Text = re.sub(r\"http\\S+\", \"\", Text)\n",
    "    # Menghapus hashtag\n",
    "    Text = re.sub(r'#', '', Text)\n",
    "    # Menghapus spasi, tanda kutip ganda (\"), dan tanda kutip tunggal (') dari teks\n",
    "    Text = Text.strip(' \"\\'')\n",
    "    # Mengganti beberapa spasi dengan satu spasi\n",
    "    Text = re.sub(r'\\s+', ' ', Text)\n",
    "    # Menghapus tanda baca\n",
    "    Text = Text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Menghapus karakter tidak diinginkan menggunakan kamus pengganti khusus jika disediakan\n",
    "    if replacements:\n",
    "        for old, new in replacements.items():\n",
    "            Text = Text.replace(old, new)\n",
    "    # Mengembalikan teks yang telah dibersihkan\n",
    "    return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09186d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       malu sama kaleng sarden dan hongguan yg bisa t...\n",
       "1                           Berguna kagak membodohi iya ðŸ’€\n",
       "2                                Berhentikan rangka esafðŸ¤¬\n",
       "3          kuning di rangka esaf itu EMAS TAI bukan KARAT\n",
       "4                 Mana penjelasan honda jngn lepas tangan\n",
       "                              ...                        \n",
       "3410    di bikin las sambungan saja bang pakai Besi PI...\n",
       "3411                     Kau ini klo ngomong benar kali ðŸ˜™\n",
       "3412    Pcx gen bro sma rambut lu aja msih tebel rambu...\n",
       "3413    Kejadian yang saya rasain pas dulu pasang stik...\n",
       "3414    Udah sering lihat motot honda beat ahm patah p...\n",
       "Name: textClean_cleaning, Length: 3344, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['textClean_cleaning'] = df['description'].apply(cleaning)\n",
    "df.drop_duplicates(subset=[\"textClean_cleaning\"], keep=\"first\", inplace=True)\n",
    "df['textClean_cleaning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc363ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       malu sama kaleng sarden dan hongguan yg bisa t...\n",
       "1                           Berguna kagak membodohi iya ðŸ’€\n",
       "2                                Berhentikan rangka esafðŸ¤¬\n",
       "3          kuning di rangka esaf itu EMAS TAI bukan KARAT\n",
       "4                 Mana penjelasan honda jngn lepas tangan\n",
       "                              ...                        \n",
       "4505    Pcx gen bro sma rambut lu aja msih tebel rambu...\n",
       "4506    Kejadian yang saya rasain pas dulu pasang stik...\n",
       "4507    Hee udah lama om Orangnya aja kelewat tutup ma...\n",
       "4508    MAU UNTUNG AJA LU MANA TANGGUNG JAWABMU LU KIR...\n",
       "4509    Udah sering lihat motot honda beat ahm patah p...\n",
       "Name: textClean_cleaning_manual, Length: 4510, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['textClean_cleaning_manual'] = df_manual['text'].apply(cleaning)\n",
    "df_manual['textClean_cleaning_manual']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33cf40",
   "metadata": {},
   "source": [
    "# Case Folding, Clean Emoji\n",
    "Merubah huruf kapital menjadi huruf kecil dan membersihkan emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4afc14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expression (regex)\n",
    "import re\n",
    "\n",
    "def casefolding(text):\n",
    "  # Mengubah teks ke huruf kecil (lowercase)\n",
    "  text = text.lower()\n",
    "  # Menghapus emoticon\n",
    "  # Pola regex untuk mendeteksi berbagai karakter emoticon dan simbol\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "  # Mengganti semua emoticon dan simbol yang terdeteksi dengan string kosong\n",
    "  text = emoji_pattern.sub(r'', text)\n",
    "  # Menghapus karakter non-ASCII\n",
    "  encoded_string = text.encode(\"ascii\", \"ignore\")\n",
    "  # Mengubah kembali byte string menjadi string normal\n",
    "  text = encoded_string.decode()\n",
    "  # Mengembalikan teks yang sudah dibersihkan\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302cc951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       malu sama kaleng sarden dan hongguan yg bisa t...\n",
       "1                            berguna kagak membodohi iya \n",
       "2                                 berhentikan rangka esaf\n",
       "3          kuning di rangka esaf itu emas tai bukan karat\n",
       "4                 mana penjelasan honda jngn lepas tangan\n",
       "                              ...                        \n",
       "3410    di bikin las sambungan saja bang pakai besi pi...\n",
       "3411                      kau ini klo ngomong benar kali \n",
       "3412    pcx gen bro sma rambut lu aja msih tebel rambut  \n",
       "3413    kejadian yang saya rasain pas dulu pasang stik...\n",
       "3414    udah sering lihat motot honda beat ahm patah p...\n",
       "Name: textClean_casefolding, Length: 3344, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['textClean_casefolding'] = df['textClean_cleaning'].apply(casefolding)\n",
    "df['textClean_casefolding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a3d4c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       malu sama kaleng sarden dan hongguan yg bisa t...\n",
       "1                            berguna kagak membodohi iya \n",
       "2                                 berhentikan rangka esaf\n",
       "3          kuning di rangka esaf itu emas tai bukan karat\n",
       "4                 mana penjelasan honda jngn lepas tangan\n",
       "                              ...                        \n",
       "4505    pcx gen bro sma rambut lu aja msih tebel rambut  \n",
       "4506    kejadian yang saya rasain pas dulu pasang stik...\n",
       "4507    hee udah lama om orangnya aja kelewat tutup ma...\n",
       "4508    mau untung aja lu mana tanggung jawabmu lu kir...\n",
       "4509    udah sering lihat motot honda beat ahm patah p...\n",
       "Name: textClean_casefolding_manual, Length: 4510, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['textClean_casefolding_manual'] = df_manual['textClean_cleaning_manual'].apply(casefolding)\n",
    "df_manual['textClean_casefolding_manual']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093bd976",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "kata yang memiliki imbuhan menjadi kata dasar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8fb409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27ddb64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       malu sama kaleng sarden dan hongguan yg bisa t...\n",
       "1                                    guna kagak bodoh iya\n",
       "2                                       henti rangka esaf\n",
       "3          kuning di rangka esaf itu emas tai bukan karat\n",
       "4                      mana jelas honda jngn lepas tangan\n",
       "                              ...                        \n",
       "3410    di bikin las sambung saja bang pakai besi pipa...\n",
       "3411                       kau ini klo ngomong benar kali\n",
       "3412      pcx gen bro sma rambut lu aja msih tebel rambut\n",
       "3413    jadi yang saya rasain pas dulu pasang stiker o...\n",
       "3414    udah sering lihat motot honda beat ahm patah p...\n",
       "Name: textClean_stemmer, Length: 3344, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['textClean_stemmer'] = df['textClean_casefolding'].apply(stemmer.stem)\n",
    "df['textClean_stemmer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc64d0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       malu sama kaleng sarden dan hongguan yg bisa t...\n",
       "1                                    guna kagak bodoh iya\n",
       "2                                       henti rangka esaf\n",
       "3          kuning di rangka esaf itu emas tai bukan karat\n",
       "4                      mana jelas honda jngn lepas tangan\n",
       "                              ...                        \n",
       "4505      pcx gen bro sma rambut lu aja msih tebel rambut\n",
       "4506    jadi yang saya rasain pas dulu pasang stiker o...\n",
       "4507    hee udah lama om orang aja lewat tutup mata sa...\n",
       "4508    mau untung aja lu mana tanggung jawab lu kira ...\n",
       "4509    udah sering lihat motot honda beat ahm patah p...\n",
       "Name: textClean_stemmer_manual, Length: 4510, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['textClean_stemmer_manual'] = df_manual['textClean_casefolding_manual'].apply(stemmer.stem)\n",
    "df_manual['textClean_stemmer_manual']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b2f39",
   "metadata": {},
   "source": [
    "# Slangwords\n",
    "mengindentifikasi kata-kata slang (kata gaul) lalu akan diganti ke kata yang lebih baku atau umum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a553a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca file CSV 'kbba.txt' yang berisi kamus kata slang dan formalnya\n",
    "kbba_dictionary = pd.read_csv('kbba.txt', delimiter='\\t', names=['slang', 'formal'], header=None, encoding='utf-8')\n",
    "# Membuat kamus dari DataFrame 'kbba_dictionary', di mana 'slang' adalah key dan 'formal' adalah value\n",
    "slang_dict = dict(zip(kbba_dictionary['slang'], kbba_dictionary['formal']))\n",
    "# Menampilkan DataFrame 'kbba_dictionary' untuk melihat isi kamus slang dan formal\n",
    "kbba_dictionary\n",
    "\n",
    "def convert_slangword(text):\n",
    "    words = text.split()\n",
    "    normalized_words = [slang_dict[word] if word in slang_dict else word for word in words]\n",
    "    normalized_text = ' '.join(normalized_words)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39758bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       malu sama kaleng sarden dan kongguan yang bisa...\n",
       "1                                 berguna tidak bodoh iya\n",
       "2                                       henti rangka esaf\n",
       "3       kuning di rangka esaf itu emas kotoran bukan k...\n",
       "4                    mana jelas honda jangan lepas tangan\n",
       "                              ...                        \n",
       "3410    di bikin las sambung saja bang pakai besi pipa...\n",
       "3411                      kamu ini kalo bicara benar kali\n",
       "3412    pcx gen saudara laki-laki sama rambut kamu saj...\n",
       "3413    jadi yang saya rasain pas dulu pasang stiker o...\n",
       "3414    sudah sering lihat motot honda beat ahm patah ...\n",
       "Name: textClean_slang, Length: 3344, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['textClean_slang'] = df['textClean_stemmer'].apply(convert_slangword)\n",
    "df['textClean_slang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3adff904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       malu sama kaleng sarden dan kongguan yang bisa...\n",
       "1                                 berguna tidak bodoh iya\n",
       "2                                       henti rangka esaf\n",
       "3       kuning di rangka esaf itu emas kotoran bukan k...\n",
       "4                    mana jelas honda jangan lepas tangan\n",
       "                              ...                        \n",
       "4505    pcx gen saudara laki-laki sama rambut kamu saj...\n",
       "4506    jadi yang saya rasain pas dulu pasang stiker o...\n",
       "4507    hee sudah lama om orang saja lewat tutup mata ...\n",
       "4508    mau untung saja kamu mana tanggung jawab kamu ...\n",
       "4509    sudah sering lihat motot honda beat ahm patah ...\n",
       "Name: textClean_slang_manual, Length: 4510, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['textClean_slang_manual'] = df_manual['textClean_stemmer_manual'].apply(convert_slangword)\n",
    "df_manual['textClean_slang_manual']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263f7f2",
   "metadata": {},
   "source": [
    "# Stopword\n",
    "ini akan menghapus seluruh kata yang dianggap tidak penting, yang mana tidak akan mempengaruhi sentimen pada kalimat. Kata yang tidak penting disini adalah kata sambung seperti, di, ke, ini, dan, dari, segala, itu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "423bf432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modul StopWord dari library nlp_id untuk penghapusan stopwords\n",
    "from nlp_id.stopword import StopWord\n",
    "# Inisialisasi objek StopWord untuk pengolahan stopwords bahasa Indonesia\n",
    "stopword = StopWord()\n",
    "# Dapatkan daftar stopwords dari modul StopWord\n",
    "stopwords = stopword.get_stopword()\n",
    "\n",
    "# Hapus kata \"tidak\" dari daftar stopwords\n",
    "if \"tidak\" in stopwords:\n",
    "    stopwords.remove(\"tidak\")\n",
    "# Fungsi untuk menghapus stopwords dengan pengecualian kata \"tidak\"\n",
    "def remove_stopwords_with_exception(text):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word not in stopwords]\n",
    "    return \" \".join(cleaned_words).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1189a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    malu kaleng sarden kongguan tahan ah\n",
       "1                                     berguna tidak bodoh\n",
       "2                                       henti rangka esaf\n",
       "3                   kuning rangka esaf emas kotoran karat\n",
       "4                                      honda lepas tangan\n",
       "                              ...                        \n",
       "3410    bikin las sambung bang pakai besi pipa galvani...\n",
       "3411                                          kalo bicara\n",
       "3412    pcx gen saudara laki-laki rambut msih tebel ra...\n",
       "3413    rasain pasang stiker oracal honda cbr dealer c...\n",
       "3414          lihat motot honda beat ahm patah porek depa\n",
       "Name: Text_Clean, Length: 3308, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gunakan fungsi remove_stopwords_with_exception pada kolom \"textClean_slang\"\n",
    "df[\"Text_Clean\"] = df[\"textClean_slang\"].apply(remove_stopwords_with_exception)\n",
    "df = df[df['Text_Clean'].str.strip().astype(bool)]\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "df[\"Text_Clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dac926a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    malu kaleng sarden kongguan tahan ah\n",
       "1                                     berguna tidak bodoh\n",
       "2                                       henti rangka esaf\n",
       "3                   kuning rangka esaf emas kotoran karat\n",
       "4                                      honda lepas tangan\n",
       "                              ...                        \n",
       "4505    pcx gen saudara laki-laki rambut msih tebel ra...\n",
       "4506    rasain pasang stiker oracal honda cbr dealer c...\n",
       "4507                         hee om orang tutup mata merk\n",
       "4508    untung tanggung bayar daun mangga kualitas jag...\n",
       "4509          lihat motot honda beat ahm patah porek depa\n",
       "Name: Text_Clean_manual, Length: 4510, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gunakan fungsi remove_stopwords_with_exception pada kolom \"textClean_slang\"\n",
    "df_manual[\"Text_Clean_manual\"] = df_manual[\"textClean_slang_manual\"].apply(remove_stopwords_with_exception)\n",
    "df_manual[\"Text_Clean_manual\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4bee6d",
   "metadata": {},
   "source": [
    "# Unwanted Word Removal\n",
    "menghapus Kata-kata yang tidak diinginkan yang akan dihapus dari teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31370e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yuki Prasetya\n",
      "[nltk_data]     Mukmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK untuk tokenisasi\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "#Unduh dataset 'punkt' dari NLTK untuk tokenisasi teks\n",
    "nltk.download('punkt')\n",
    "\n",
    "# menghapus Kata-kata yang tidak diinginkan yang akan dihapus dari teks\n",
    "unwanted_words = ['jan','feb','mar','apr','mei','jun','jul','aug','sep','oct','nov','dec','uaddown','weareuad','Iam','https','igshid']\n",
    "\n",
    "def RemoveUnwantedwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [word for word in word_tokens if not word in unwanted_words]\n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64f72390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    malu kaleng sarden kongguan tahan ah\n",
       "1                                     berguna tidak bodoh\n",
       "2                                       henti rangka esaf\n",
       "3                   kuning rangka esaf emas kotoran karat\n",
       "4                                      honda lepas tangan\n",
       "                              ...                        \n",
       "3410    bikin las sambung bang pakai besi pipa galvani...\n",
       "3411                                          kalo bicara\n",
       "3412    pcx gen saudara laki-laki rambut msih tebel ra...\n",
       "3413    rasain pasang stiker oracal honda cbr dealer c...\n",
       "3414          lihat motot honda beat ahm patah porek depa\n",
       "Name: Text_Clean_lagi, Length: 3308, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text_Clean_lagi\"] = df[\"Text_Clean\"].apply(RemoveUnwantedwords)\n",
    "df[\"Text_Clean_lagi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "962bfc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    malu kaleng sarden kongguan tahan ah\n",
       "1                                     berguna tidak bodoh\n",
       "2                                       henti rangka esaf\n",
       "3                   kuning rangka esaf emas kotoran karat\n",
       "4                                      honda lepas tangan\n",
       "                              ...                        \n",
       "4505    pcx gen saudara laki-laki rambut msih tebel ra...\n",
       "4506    rasain pasang stiker oracal honda cbr dealer c...\n",
       "4507                         hee om orang tutup mata merk\n",
       "4508    untung tanggung bayar daun mangga kualitas jag...\n",
       "4509          lihat motot honda beat ahm patah porek depa\n",
       "Name: Text_Clean_lagi_manual, Length: 4510, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual[\"Text_Clean_lagi_manual\"] = df_manual[\"Text_Clean_manual\"].apply(RemoveUnwantedwords)\n",
    "df_manual[\"Text_Clean_lagi_manual\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca896e",
   "metadata": {},
   "source": [
    "# Menghapus kata di bawah 3 huruf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd881278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       malu kaleng sarden kongguan tahan\n",
       "1                                     berguna tidak bodoh\n",
       "2                                       henti rangka esaf\n",
       "3                   kuning rangka esaf emas kotoran karat\n",
       "4                                      honda lepas tangan\n",
       "                              ...                        \n",
       "3410    bikin las sambung bang pakai besi pipa galvani...\n",
       "3411                                          kalo bicara\n",
       "3412    pcx gen saudara laki laki rambut msih tebel ra...\n",
       "3413    rasain pasang stiker oracal honda cbr dealer c...\n",
       "3414          lihat motot honda beat ahm patah porek depa\n",
       "Name: Text_Clean_lagi, Length: 3308, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text_Clean_lagi\"] = df[\"Text_Clean_lagi\"].str.findall('\\w{3,}').str.join(' ')\n",
    "df[\"Text_Clean_lagi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "630f4865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       malu kaleng sarden kongguan tahan\n",
       "1                                     berguna tidak bodoh\n",
       "2                                       henti rangka esaf\n",
       "3                   kuning rangka esaf emas kotoran karat\n",
       "4                                      honda lepas tangan\n",
       "                              ...                        \n",
       "4505    pcx gen saudara laki laki rambut msih tebel ra...\n",
       "4506    rasain pasang stiker oracal honda cbr dealer c...\n",
       "4507                            hee orang tutup mata merk\n",
       "4508    untung tanggung bayar daun mangga kualitas jag...\n",
       "4509          lihat motot honda beat ahm patah porek depa\n",
       "Name: Text_Clean_lagi_manual, Length: 4510, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual[\"Text_Clean_lagi_manual\"] = df_manual[\"Text_Clean_lagi_manual\"].str.findall('\\w{3,}').str.join(' ')\n",
    "df_manual[\"Text_Clean_lagi_manual\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d78e7e",
   "metadata": {},
   "source": [
    "# Tokenizing\n",
    "proses pemisahan kata, pemisahan kata pada setiap kalimat dilakukan berdasarkan delimeter yaitu adanya spasi pada setiap kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ea1d1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yuki Prasetya\n",
      "[nltk_data]     Mukmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Mengimpor modul NLTK untuk pengolahan bahasa alami\n",
    "import nltk\n",
    "# Mengimpor fungsi word_tokenize dari NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Mengunduh dataset 'punkt' dari NLTK yang digunakan untuk tokenisasi kata\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(teks):\n",
    "    list_teks = []\n",
    "    words = teks.split(\" \")\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] == \"tidak\" and i + 1 < len(words) and words[i + 1] == \"percaya\":\n",
    "            list_teks.append(\"tidak percaya\")\n",
    "            i += 2\n",
    "        else:\n",
    "            list_teks.append(words[i])\n",
    "            i += 1\n",
    "    return list_teks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83df9e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [malu, kaleng, sarden, kongguan, tahan]\n",
       "1                         [berguna, tidak, bodoh]\n",
       "2                           [henti, rangka, esaf]\n",
       "3    [kuning, rangka, esaf, emas, kotoran, karat]\n",
       "4                          [honda, lepas, tangan]\n",
       "Name: Text_Clean_split, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text_Clean_split\"] = df[\"Text_Clean_lagi\"].apply(tokenize)\n",
    "df[\"Text_Clean_split\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bc73a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [malu, kaleng, sarden, kongguan, tahan]\n",
       "1                         [berguna, tidak, bodoh]\n",
       "2                           [henti, rangka, esaf]\n",
       "3    [kuning, rangka, esaf, emas, kotoran, karat]\n",
       "4                          [honda, lepas, tangan]\n",
       "Name: Text_Clean_split_manual, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual[\"Text_Clean_split_manual\"] = df_manual[\"Text_Clean_lagi_manual\"].apply(tokenize)\n",
    "df_manual[\"Text_Clean_split_manual\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bfd598",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "471c11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export Data\n",
    "# from datetime import datetime\n",
    "# # Get the current date and time\n",
    "# current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# # Create the filename with the current date and time\n",
    "# filename = f'hasil_preprocessing_komentar_youtube_{current_time}.xlsx'\n",
    "# # Export DataFrame to Excel\n",
    "# df.to_excel(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5f9c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export Data\n",
    "# from datetime import datetime\n",
    "# # Get the current date and time\n",
    "# current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# # Create the filename with the current date and time\n",
    "# filename = f'hasil_labelling_manual.xlsx'\n",
    "# # Export DataFrame to Excel\n",
    "# df_manual.to_excel(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f7025",
   "metadata": {},
   "source": [
    "# Labeling Data (lexicon)\n",
    "proses labeling dengan mencocokkan dan menghitung banyaknya kata positif dan negatif sesuai dengan kamus, lalu data akandihitung jika lebih banyak kata positif dari pada kata negatif maka akan diberikan label positif dan jika lebih banyak kata negatif dari pada kata positif maka akan diberikan label negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "235d35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar Kata-kata positif bahasa indonesia\n",
    "df_positive = pd.read_csv('https://raw.githubusercontent.com/YUKIPM24/kamus_lexicon/main/positive.txt', sep='\\t',names=['positive'])\n",
    "list_positive = list(df_positive.iloc[::,0])\n",
    "\n",
    "# Daftar Kata-Kata negatif bahasa indonesia\n",
    "df_negative = pd.read_csv('https://raw.githubusercontent.com/YUKIPM24/kamus_lexicon/main/negative.txt', sep='\\t',names=['negative'])\n",
    "list_negative = list(df_negative.iloc[::,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc5788aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghitung kata-kata positif/negatif pada dictionary lalu menentukan sentimennya :\n",
    "def sentiment_kamus_lexicon_id(text):\n",
    "    score = 0\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "    neutral_words = []\n",
    "    \n",
    "    for word in text:\n",
    "        if word in list_positive:\n",
    "            score += 1\n",
    "            positive_words.append(word)\n",
    "        elif word in list_negative:\n",
    "            score -= 1\n",
    "            negative_words.append(word)\n",
    "        else:\n",
    "            neutral_words.append(word)\n",
    "    \n",
    "    polarity = ''\n",
    "    if score > 0:\n",
    "        polarity = 'positive'\n",
    "    elif score < 0:\n",
    "        polarity = 'negative'\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "    \n",
    "    result = {'positif': positive_words, 'negatif': negative_words, 'neutral': neutral_words}\n",
    "    return score, polarity, result, positive_words, negative_words, neutral_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e167cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_manual_label(text_labels):\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "    neutral_words = []\n",
    "    \n",
    "    for word, label in text_labels:\n",
    "        if label.lower() == 'positive':\n",
    "            positive_words.append(word)\n",
    "        elif label.lower() == 'negative':\n",
    "            negative_words.append(word)\n",
    "        else:\n",
    "            neutral_words.append(word)\n",
    "    \n",
    "    total_positive = len(positive_words)\n",
    "    total_negative = len(negative_words)\n",
    "    total_neutral = len(neutral_words)\n",
    "    \n",
    "    score = total_positive - total_negative\n",
    "    \n",
    "    polarity = ''\n",
    "    if score > 0:\n",
    "        polarity = 'positive'\n",
    "    elif score < 0:\n",
    "        polarity = 'negative'\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "    \n",
    "    result = {'positif': positive_words, 'negatif': negative_words, 'netral': neutral_words}\n",
    "    return score, polarity, result, positive_words, negative_words, neutral_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1f0e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil = df['Text_Clean_split'].apply(sentiment_kamus_lexicon_id)\n",
    "hasil = list(zip(*hasil))\n",
    "df['polarity_score'] = hasil[0]\n",
    "df['polarity'] = hasil[1]\n",
    "hasil_kata_positive = hasil[3]\n",
    "hasil_kata_negative = hasil[4]\n",
    "\n",
    "df = df[df.polarity != 'neutral']\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd88eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['sentiment_label'] = df_manual['sentiment_label'].replace({'Positif': 'positive', 'Negatif': 'negative', 'Netral': 'neutral'})\n",
    "\n",
    "# Gabungkan Text_Clean_split_manual dengan sentiment_label menjadi satu daftar tuple\n",
    "df_manual['text_labels'] = df_manual.apply(lambda row: list(zip(row['Text_Clean_split_manual'], row['sentiment_label'].split())), axis=1)\n",
    "\n",
    "# Terapkan fungsi sentiment analysis ke kolom text_labels\n",
    "df_manual['sentiment_analysis'] = df_manual['text_labels'].apply(sentiment_manual_label)\n",
    "\n",
    "# Pisahkan hasil menjadi kolom-kolom terpisah\n",
    "hasil = list(zip(*df_manual['sentiment_analysis']))\n",
    "df_manual['polarity_score'] = hasil[0]\n",
    "df_manual['polarity'] = hasil[1]\n",
    "df_manual['result'] = hasil[2]\n",
    "df_manual['positive_words'] = hasil[3]\n",
    "df_manual['negative_words'] = hasil[4]\n",
    "df_manual['neutral_words'] = hasil[5]\n",
    "\n",
    "# Filter DataFrame hanya untuk sentimen yang bukan netral\n",
    "df_manual = df_manual[df_manual['polarity'] != 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69633055-528f-4c7c-b0a8-933ee4a11973",
   "metadata": {},
   "source": [
    "## Merge data hasil labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96c2e0d7-1423-48b0-9378-d6a5a9f549ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1749, 4510)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(df_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37e0344b-f6e3-4228-a85f-40e2aa8f7f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label_lexicon</th>\n",
       "      <th>sentiment_label_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>malu kaleng sarden kongguan tahan</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>berguna tidak bodoh</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>henti rangka esaf</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>karat salah orang produksi salah coba produksi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>takut muka dasar honda esaf</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment_label_lexicon  \\\n",
       "0                  malu kaleng sarden kongguan tahan                negative   \n",
       "1                                berguna tidak bodoh                negative   \n",
       "2                                  henti rangka esaf                negative   \n",
       "6  karat salah orang produksi salah coba produksi...                negative   \n",
       "7                        takut muka dasar honda esaf                negative   \n",
       "\n",
       "  sentiment_label_manual  \n",
       "0               negative  \n",
       "1               negative  \n",
       "2               negative  \n",
       "6               negative  \n",
       "7               negative  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merge = {\n",
    "    \"text\": df[\"Text_Clean_lagi\"],\n",
    "    \"sentiment_label_lexicon\": df[\"polarity\"],\n",
    "    \"sentiment_label_manual\": df_manual[\"polarity\"]\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data_merge)\n",
    "data.dropna(inplace=True, axis=0)\n",
    "data.to_excel(\"hasil_labeling_lexicon_manual.xlsx\",index=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ea2f3b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah data: \n",
      "1749\n",
      "\n",
      "detail :\n",
      "polarity\n",
      "negative    945\n",
      "positive    804\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# menghitung hasil sentiment analysis\n",
    "print(\"jumlah data: \")\n",
    "print(df['polarity'].value_counts().sum())\n",
    "\n",
    "print(\"\\ndetail :\")\n",
    "print(df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fdc90e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data:\n",
      "4510\n",
      "\n",
      "Detail:\n",
      "polarity\n",
      "negative    3239\n",
      "positive    1271\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Menghitung jumlah data\n",
    "print(\"Jumlah data:\")\n",
    "print(df_manual['polarity'].value_counts().sum())\n",
    "\n",
    "# Menampilkan detail sentimen\n",
    "print(\"\\nDetail:\")\n",
    "print(df_manual['polarity'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
